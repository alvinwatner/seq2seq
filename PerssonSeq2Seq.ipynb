{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PerssonSeq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRxMalLqnlIO"
      },
      "source": [
        "#<div align=\"center\">Sequence to Sequence</div>\n",
        "\n",
        "---\n",
        "###### Author     : alvinwatner\n",
        "###### Inspired by : Aladdin Persson (https://www.youtube.com/watch?v=EoGUlvhRYpk&t=2146s)\n",
        "---\n",
        "\n",
        "##To reproduce : \n",
        "\n",
        "1.   Make sure you have utils.py inside your google drive, otherwise there are 2 ways to acquire utils.py file :\n",
        "\n",
        "  *   Open and Run the [utils.ipynb](https://colab.research.google.com/drive/1tkh9NyoKLfl4gk8L3_JKVGfCMIXDLy37?usp=sharing).\n",
        "  *   Download from this [link](https://drive.google.com/file/d/18RMp42VvfMSdUfxxYKR_1QiA5fWcbgus/view?usp=sharing).\n",
        "\n",
        "  Also make sure the **utils.py** is inside the **main_path** directory that specified on the *second cell*.\n",
        "\n",
        "\n",
        "2. Create \"**Checkpoint**\" folder inside the **main_path**.\n",
        "3. Create \"**plot**\" folder inside the **main_path** .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "###### *Note : Don't forget to change runtime type to **GPU**, and **CTRL + F9** to run all cells, otherwise the training will run super slow. Enjoy~*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVDR37uU2t_Y",
        "outputId": "01daae35-391e-4169-9aee-679d8b59bdb9"
      },
      "source": [
        "# Mount to gdrive and have a peek to current path\n",
        "from google.colab import drive \n",
        "drive. mount('/content/drive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYpNaxOt3Se1"
      },
      "source": [
        "# Main path for tensorboard and checkpoint\n",
        "main_path = '/content/drive/MyDrive/Colab Notebooks/NLP/Pytorch/Seq2seq/'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8P6Eqgq3YeA",
        "outputId": "a9d029d0-6938-4cc0-ee00-9ee8d87ccbed"
      },
      "source": [
        "# Path to utils.py root folder\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/NLP/Pytorch/Seq2seq"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/NLP/Pytorch/Seq2seq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io5Y5XQG3bUy",
        "outputId": "02289639-c6b3-43ed-a888-b69627c30966"
      },
      "source": [
        "# Make sure the utils.py is inside the main_path directory that specified on the second cell.\n",
        "!ls -l utils.py"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 2415 Dec 23 03:36 utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdTd06nb3d-0"
      },
      "source": [
        "#import utils.py as module\n",
        "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU3Hgn8e3f5g"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import numpy as np \n",
        "import random\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import datetime\n",
        "import os"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYNxfF-L3jvW",
        "outputId": "d771a615-18ac-4d30-de82-4266a8e1d6b7"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"device = {device}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egL1TR6A2X62"
      },
      "source": [
        "!pip install torchtext==0.6.0 \n",
        "!python -m spacy download de #Language degerman\n",
        "!python -m spacy download en #Language enlish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJtGmxPo3k3o"
      },
      "source": [
        "spacy_ger = spacy.load('de')\n",
        "spacy_en  = spacy.load('en')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k63fc6Iu3oo5"
      },
      "source": [
        "def tokenizer_ger(text):\n",
        "  return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "\n",
        "def tokenizer_en(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op-zMKmI4Hq9"
      },
      "source": [
        "gerField = Field(tokenize = tokenizer_ger, lower = True, \\\n",
        "                 init_token = '<SOS>', eos_token = '<EOS>')\n",
        "\n",
        "enField = Field(tokenize = tokenizer_en, lower = True, \\\n",
        "                 init_token = '<SOS>', eos_token = '<EOS>')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlxiZ3Pi4Xpq"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (gerField, enField))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S04T1_lX4sXq"
      },
      "source": [
        "gerField.build_vocab(train_data, max_size = 10000, min_freq = 2)\n",
        "enField.build_vocab(train_data, max_size = 10000, min_freq = 2)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGqt8u705Cs3"
      },
      "source": [
        "encoder_input_size = len(gerField.vocab)\n",
        "decoder_input_size = len(enField.vocab)\n",
        "num_layers = 2\n",
        "hidden_size = 1024\n",
        "batch_size = 32\n",
        "embedding_dim = 300\n",
        "dropout = 0.5\n",
        "num_epochs = 10"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R-rM-Aa5GhU"
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x : len(x.src),\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbqWbahH5Ild"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_dim, hidden_size, num_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding  = nn.Embedding(input_size, embedding_dim)\n",
        "    self.rnn        = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout = dropout)\n",
        "    self.dropout    = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, inp):\n",
        "    #inp.shape         = (seq_length, batch_size)\n",
        "    embedded           = self.dropout(self.embedding(inp))\n",
        "    #embedded.shape    = (seq_length, batch_size, embedding_dim) \n",
        "    _ , (hidden, cell) = self.rnn(embedded)\n",
        "    #hidden.shape      = (num_layers, batch_size, embedding_dim)\n",
        "    #cell.shape        = (num_layers, batch_size, embedding_dim)\n",
        "    return hidden, cell"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQmVyiVl68Lw"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_dim, hidden_size, num_layers, dropout):\n",
        "    super().__init__()\n",
        "    vocab_size      = input_size\n",
        "    self.embedding  = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.rnn        = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout = dropout)\n",
        "    self.fc         = nn.Linear(hidden_size, vocab_size)\n",
        "    self.dropout    = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, inp, hidden, cell):\n",
        "    #inp.shape             = (batch_size,)\n",
        "    inp                    = inp.unsqueeze(0)\n",
        "    #inp.shape             = (1, batch_size)\n",
        "    embedded               = self.dropout(self.embedding(inp) )\n",
        "    #embedded.shape        = (1, batch_size, embedding_dim)\n",
        "    out_de, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "    #out_de                = (1, batch_size, hidden_size)\n",
        "    #hidden                = (num_layers, batch_size, hidden_size)\n",
        "    #cell                  = (num_layers, batch_size, hidden_size)\n",
        "\n",
        "    out_de                 = self.fc(out_de)\n",
        "    #out_de                = (1, batch_size, vocab_size)\n",
        "    out_de                 = out_de.squeeze(0)\n",
        "    #out_de                = (1, batch_size, vocab_size)\n",
        "\n",
        "    return out_de, hidden, cell\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THCyM7Gy7QRK"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder \n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, source, target, tfr = 0.5):  \n",
        "    #source.shape = (seq_length, batch_size) \n",
        "    #target.shape = (seq_length, batch_size) \n",
        "    seq_length    = target.shape[0]\n",
        "    batch_size    = target.shape[1]\n",
        "    vocab_size    = len(enField.vocab)\n",
        "    outputs       = torch.zeros(seq_length, batch_size, vocab_size).to(device)\n",
        "\n",
        "    # init_token = <SOS>\n",
        "    inp = target[0]\n",
        "    #inp.shape = (32,)\n",
        "\n",
        "    hidden, cell = self.encoder(source)\n",
        "    \n",
        "    for t in range(1, seq_length):\n",
        "      output, hidden, cell = self.decoder(inp, hidden, cell)\n",
        "      #output.shape        = (batch_size, vocab_size)\n",
        "      #hidden.shape        = (batch_size, vocab_size) \n",
        "\n",
        "      outputs[t] = output\n",
        "\n",
        "      best_guess = output.argmax(1)\n",
        "      \n",
        "      inp = best_guess if random.random() > tfr else target[t]\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB55i3LoBAUZ"
      },
      "source": [
        "encoder = Encoder(encoder_input_size, embedding_dim, hidden_size, num_layers, dropout).to(device)\n",
        "decoder = Decoder(decoder_input_size, embedding_dim, hidden_size, num_layers, dropout).to(device)\n",
        "seq2seq = Seq2Seq(encoder, decoder).to(device)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdEVQv1oBrBN"
      },
      "source": [
        "pad_idx   = enField.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
        "optimizer = optim.Adam(seq2seq.parameters(), lr = 0.001)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Ww-_FMCZv4"
      },
      "source": [
        "plot_ = {'avg_loss' : [], 'epoch' : []}"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHWtJJchCeZ3"
      },
      "source": [
        "load_model = False\n",
        "\n",
        "if load_model:\n",
        "  model_name    = 'PerssonSeq2seq'\n",
        "  current_epoch = 0\n",
        "  path = f\"{main_path}/Checkpoint/|Model_{model_name}|Epoch_{current_epoch}.pth.tar\"\n",
        "  load_checkpoint(torch.load(path), seq2seq, optimizer)\n",
        "else:\n",
        "  model_name = 'Seq2Seq'\n",
        "  current_epoch = 0"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5QQErDrJnIm"
      },
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "time_zone = pytz.timezone('Asia/Jakarta')\n",
        "\n",
        "def delta_time(start_time, stop_time):\n",
        "  delta_time = stop_time - start_time\n",
        "  minute, second = divmod(delta_time.total_seconds(), 60)  \n",
        "  hour, minute   = divmod(minute, 60)  \n",
        "\n",
        "  return int(hour), int(minute), int(second)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3JuuU_sCmHp",
        "outputId": "283ce233-0eae-4a9f-8401-c2cf3bef2519"
      },
      "source": [
        "start_time = datetime.datetime.now(time_zone)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss     = 0\n",
        "  iteration_loss = [] \n",
        "\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    source = batch.src.to(device)\n",
        "    target = batch.trg.to(device)\n",
        "\n",
        "    outputs = seq2seq(source, target)\n",
        "    outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
        "    ground_Truth = target[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, ground_Truth)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm(seq2seq.parameters(), max_norm = 1)\n",
        "    optimizer.step()\n",
        "    loss_val = loss.item()\n",
        "    iteration_loss.append(loss_val)\n",
        "\n",
        "    epoch_loss += loss_val\n",
        "\n",
        "    # Average loss for the last 100 iteration         \n",
        "    if not batch_idx % 100 and batch_idx != 0:\n",
        "      print(f\"EPOCH : {epoch}/{num_epochs - 1} || Batch Iteration : {batch_idx}/{len(train_iterator)} || Avg_Loss from {batch_idx - 100} to {batch_idx} : {sum(iteration_loss[-100:])/100}\") \n",
        "  \n",
        "  \n",
        "  avg_loss = epoch_loss/len(train_iterator)\n",
        "  plot_['avg_loss'].append(avg_loss)\n",
        "  plot_['epoch'].append(epoch)\n",
        "\n",
        "  # Save Current Checkpoint and Plot\n",
        "  checkpoint = {'state_dict' : seq2seq.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "  save_checkpoint(checkpoint, \n",
        "                  filename=f\"{main_path}Checkpoint/|Model_{model_name}|Epoch_{current_epoch}.pth.tar\")  \n",
        "  \n",
        "  np.save(f\"{main_path}plot/AvgLoss_Model_{model_name}|Epoch_{current_epoch}\", \\\n",
        "                              np.array(plot_['avg_loss']))  \n",
        "  np.save(f\"{main_path}plot/Epoch_Model_{model_name}|Epoch_{current_epoch}\", \\\n",
        "                              np.array(plot_['epoch']))  \n",
        "\n",
        "  # Remove Previous Checkpoint and Plot\n",
        "  if epoch > 0:\n",
        "    os.remove(f\"{main_path}Checkpoint/|Model_{model_name}|Epoch_{current_epoch - 1}.pth.tar\")    \n",
        "    os.remove(f\"{main_path}plot/AvgLoss_Model_{model_name}|Epoch_{current_epoch - 1}.npy\")\n",
        "    os.remove(f\"{main_path}plot/Epoch_Model_{model_name}|Epoch_{current_epoch - 1}.npy\")\n",
        "\n",
        "  if epoch == num_epochs - 1:\n",
        "    stop_time = datetime.datetime.now(time_zone)\n",
        "    hour, minute, second = delta_time(start_time, stop_time)\n",
        "\n",
        "    print(\"\")\n",
        "    print(f\" **************  TRAINING DONE {num_epochs} EPOCHS  ************\")\n",
        "    print(f\" **********  {hour} Hours, {minute} Minutes, {second} Seconds  ********\")        \n",
        "    print(\" ====================================================\")  \n",
        "    print(f\"|              Average Loss : {avg_loss}     |\")   \n",
        "    print(\" ====================================================\") \n",
        "\n",
        "  else:\n",
        "    print(f\" ************** EPOCH - {epoch}/{num_epochs - 1} **************\")\n",
        "    print(f\"|       Average Loss : {avg_loss}           |\")   \n",
        "    print(\" ********************************************\")         \n",
        "    print(\"\")\n",
        "\n",
        "  current_epoch += 1    \n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH : 0/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 5.186486873626709\n",
            "EPOCH : 0/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 4.782468976974488\n",
            "EPOCH : 0/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 4.663192913532257\n",
            "EPOCH : 0/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 4.474164125919342\n",
            "EPOCH : 0/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 4.398435051441193\n",
            "EPOCH : 0/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 4.203316080570221\n",
            "EPOCH : 0/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 4.158723690509796\n",
            "EPOCH : 0/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 4.076728720664978\n",
            "EPOCH : 0/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 3.9998026132583617\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 0/9 **************\n",
            "|       Average Loss : 4.441307349662402           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 1/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 3.8706528472900392\n",
            "EPOCH : 1/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 3.8573349213600157\n",
            "EPOCH : 1/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 3.834089994430542\n",
            "EPOCH : 1/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 3.782979416847229\n",
            "EPOCH : 1/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 3.7354276990890503\n",
            "EPOCH : 1/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 3.6710277891159055\n",
            "EPOCH : 1/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 3.675394172668457\n",
            "EPOCH : 1/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 3.61686851978302\n",
            "EPOCH : 1/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 3.58249694108963\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 1/9 **************\n",
            "|       Average Loss : 3.736145752924674           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 2/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 3.4406613421440126\n",
            "EPOCH : 2/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 3.4276540899276733\n",
            "EPOCH : 2/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 3.4035860228538515\n",
            "EPOCH : 2/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 3.3565459942817686\n",
            "EPOCH : 2/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 3.3636444568634034\n",
            "EPOCH : 2/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 3.3568017435073854\n",
            "EPOCH : 2/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 3.3332926559448244\n",
            "EPOCH : 2/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 3.3569225573539736\n",
            "EPOCH : 2/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 3.31587810754776\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 2/9 **************\n",
            "|       Average Loss : 3.3715183574546135           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 3/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 3.105241105556488\n",
            "EPOCH : 3/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 3.1491803240776064\n",
            "EPOCH : 3/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 3.1460889530181886\n",
            "EPOCH : 3/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 3.1122127294540407\n",
            "EPOCH : 3/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 3.156306130886078\n",
            "EPOCH : 3/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 3.143564419746399\n",
            "EPOCH : 3/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 3.1555106687545775\n",
            "EPOCH : 3/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 3.1007499742507934\n",
            "EPOCH : 3/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 3.0546081018447877\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 3/9 **************\n",
            "|       Average Loss : 3.1238102760441118           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 4/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 2.8435577261447906\n",
            "EPOCH : 4/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 2.901810032129288\n",
            "EPOCH : 4/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 2.9008785319328307\n",
            "EPOCH : 4/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 2.900668420791626\n",
            "EPOCH : 4/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 2.9415843152999877\n",
            "EPOCH : 4/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 2.9399595355987547\n",
            "EPOCH : 4/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 2.9617560625076296\n",
            "EPOCH : 4/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 2.945342811346054\n",
            "EPOCH : 4/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 2.9067926061153413\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 4/9 **************\n",
            "|       Average Loss : 2.9173024302675024           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 5/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 2.685624947547913\n",
            "EPOCH : 5/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 2.7277551794052126\n",
            "EPOCH : 5/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 2.772820550203323\n",
            "EPOCH : 5/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 2.7306278347969055\n",
            "EPOCH : 5/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 2.7330889105796814\n",
            "EPOCH : 5/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 2.7349732899665833\n",
            "EPOCH : 5/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 2.751969178915024\n",
            "EPOCH : 5/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 2.724396245479584\n",
            "EPOCH : 5/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 2.7701323223114014\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 5/9 **************\n",
            "|       Average Loss : 2.7385926935349434           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 6/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 2.5114207184314727\n",
            "EPOCH : 6/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 2.5484246969223023\n",
            "EPOCH : 6/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 2.551492533683777\n",
            "EPOCH : 6/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 2.556103619337082\n",
            "EPOCH : 6/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 2.578860114812851\n",
            "EPOCH : 6/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 2.5971234810352324\n",
            "EPOCH : 6/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 2.547038165330887\n",
            "EPOCH : 6/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 2.581703587770462\n",
            "EPOCH : 6/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 2.6083984637260436\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 6/9 **************\n",
            "|       Average Loss : 2.564653463111297           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 7/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 2.3564436447620394\n",
            "EPOCH : 7/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 2.3885090947151184\n",
            "EPOCH : 7/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 2.3919448709487914\n",
            "EPOCH : 7/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 2.4401124370098115\n",
            "EPOCH : 7/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 2.4186608481407164\n",
            "EPOCH : 7/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 2.4663724160194396\n",
            "EPOCH : 7/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 2.416076514720917\n",
            "EPOCH : 7/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 2.46641135931015\n",
            "EPOCH : 7/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 2.4801067423820498\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 7/9 **************\n",
            "|       Average Loss : 2.426388407226697           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 8/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 2.17302787899971\n",
            "EPOCH : 8/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 2.2374988293647764\n",
            "EPOCH : 8/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 2.285360926389694\n",
            "EPOCH : 8/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 2.2996440935134888\n",
            "EPOCH : 8/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 2.3412056839466096\n",
            "EPOCH : 8/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 2.2990118849277494\n",
            "EPOCH : 8/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 2.324723528623581\n",
            "EPOCH : 8/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 2.329240107536316\n",
            "EPOCH : 8/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 2.317229201793671\n",
            "=> Saving checkpoint\n",
            " ************** EPOCH - 8/9 **************\n",
            "|       Average Loss : 2.29183860939682           |\n",
            " ********************************************\n",
            "\n",
            "EPOCH : 9/9 || Batch Iteration : 100/907 || Avg_Loss from 0 to 100 : 2.0948163509368896\n",
            "EPOCH : 9/9 || Batch Iteration : 200/907 || Avg_Loss from 100 to 200 : 2.163513180017471\n",
            "EPOCH : 9/9 || Batch Iteration : 300/907 || Avg_Loss from 200 to 300 : 2.1282447016239168\n",
            "EPOCH : 9/9 || Batch Iteration : 400/907 || Avg_Loss from 300 to 400 : 2.1691764986515043\n",
            "EPOCH : 9/9 || Batch Iteration : 500/907 || Avg_Loss from 400 to 500 : 2.178717269897461\n",
            "EPOCH : 9/9 || Batch Iteration : 600/907 || Avg_Loss from 500 to 600 : 2.1999385166168213\n",
            "EPOCH : 9/9 || Batch Iteration : 700/907 || Avg_Loss from 600 to 700 : 2.1855241537094114\n",
            "EPOCH : 9/9 || Batch Iteration : 800/907 || Avg_Loss from 700 to 800 : 2.214658703804016\n",
            "EPOCH : 9/9 || Batch Iteration : 900/907 || Avg_Loss from 800 to 900 : 2.2513569843769075\n",
            "=> Saving checkpoint\n",
            "\n",
            " **************  TRAINING DONE 10 EPOCHS  ************\n",
            " **********  0 Hours, 15 Minutes, 44 Seconds  ********\n",
            " ====================================================\n",
            "|              Average Loss : 2.1772984462115574     |\n",
            " ====================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ygiQ69HI56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "2b4ffe87-4d5f-4fba-adc4-4ec0a9985c4e"
      },
      "source": [
        "#source  \n",
        "plt.plot(plot_['epoch'], plot_['avg_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('avg_loss')\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}